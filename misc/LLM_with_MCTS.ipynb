{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17d17472-501b-41f3-837b-d3abb96595db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym # open ai gym\n",
    "import os,re\n",
    "import torch\n",
    "import random\n",
    "from static_env import StaticEnv\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02c583f3-0f46-464f-ba99-a77baedfd85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):                                                 \n",
    "    random.seed(seed)                                                     \n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)                                                   \n",
    "        torch.cuda.manual_seed_all(seed)                                             \n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "        #print(\"Using GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "class LLMQueryEnv(gym.Env, StaticEnv):\n",
    "    \"\"\"\n",
    "    Simple gym environment with the goal to navigate the player from its\n",
    "    starting position to the highest point on a two-dimensional map within\n",
    "    a limited number of steps. Rewards are defined as the difference in\n",
    "    altitude between states minus a penalty for each step. The player starts\n",
    "    in the lower left corner of the map and the highest point is in the upper\n",
    "    right corner. Map layout mirrors CliffWalking environment:\n",
    "    top left = (0, 0), top right = (0, m-1), bottom left = (n-1, 0),\n",
    "    bottom right = (n-1, m-1).\n",
    "    The setup of this environment was inspired by the energy landscape in\n",
    "    protein folding.\n",
    "    \"\"\"\n",
    "\n",
    "    # origAIG incomplete prompt\n",
    "    \n",
    "    def __init__(self,orig_prompt=\"def hello_world():\"):\n",
    "        seed_everything()\n",
    "        model_name = \"Salesforce/codegen-350M-multi\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "        self.orig_prompt = orig_prompt\n",
    "        self.init_state = self.get_tokenized_state(self.orig_prompt)\n",
    "        self.num_tokens=0\n",
    "        self.n_actions = 51200 #self.tokenizer.vocab_size\n",
    "        self.stopwords = ['\\n\\n']\n",
    "        self.depth=20\n",
    "\n",
    "        #self.ep_length = NUM_LENGTH_EPISODES # not required\n",
    "\n",
    "    def get_tokenized_state(self,prompt):\n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "        return input_ids.numpy()\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        state = self.init_state\n",
    "        return state\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Go back to the initial state. \"\"\"\n",
    "        state = self.init_state\n",
    "        return state\n",
    "\n",
    "    def trim_with_stopwords(self, currentState):\n",
    "        \n",
    "        # with torch.no_grad():\n",
    "        #     decoded = self.tokenizer.decode(currentState[0])\n",
    "            \n",
    "        for w in sorted(self.stopwords, key=len, reverse=True):\n",
    "            \n",
    "            if currentState.endswith(w):\n",
    "                currentState = currentState[:-len(w)]\n",
    "                # print('Trimmed', repr(currentState))\n",
    "                return currentState\n",
    "    \n",
    "    def isPromptComplete(self,currentState,depth):\n",
    "        \"\"\"Needs to be implemented\"\"\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            torchState = torch.from_numpy(currentState).to(device)\n",
    "            decoded = self.tokenizer.decode(currentState[0])\n",
    "            \n",
    "        # decoded = self.get_prompt_from_state(self,currentState)\n",
    "        print('decoded state',repr(decoded))\n",
    "        \n",
    "        for w in sorted(self.stopwords, key=len, reverse=True):\n",
    "            if decoded.endswith(w):\n",
    "                # decoded = decoded[:-len(w)]\n",
    "                # print('Trimmed', repr(decoded))\n",
    "                return True\n",
    "\n",
    "    # def getPromptScore(self,completePrompt):\n",
    "    #     \"\"\"Needs to be implemented\"\"\"\n",
    "    #     return 0.0\n",
    "\n",
    "    def getPromptScore(self,currentState):\n",
    "        \"\"\"Needs to be implemented\"\"\"\n",
    "        #print(currentState)\n",
    "        #print(self.num_tokens)\n",
    "        initScore = 0\n",
    "        if currentState[0][-1] == 4480 or currentState[0][-1] == 361 or currentState[0][-1] == 1820:\n",
    "            initScore+=0.1\n",
    "        if currentState[0][-2] == 37881 or currentState[0][-2] == 2 or currentState[0][-2] == 17772:\n",
    "            initScore+=0.1\n",
    "        if currentState[0][-3] == 1635 or currentState[0][-3] == 611 or currentState[0][-3] == 220:\n",
    "            initScore+=0.1\n",
    "        if currentState[0][-4] == 422 or currentState[0][-4] == 6 or currentState[0][-4] == 5145:\n",
    "            initScore+=0.1\n",
    "        if currentState[0][-5] == 995 or currentState[0][-5] == 422 or currentState[0][-5] == 705:\n",
    "            initScore+=0.1\n",
    "        if currentState[0][-6] == 15496 or currentState[0][-6] == 6894 or currentState[0][-6] == 10603:\n",
    "            initScore+=0.1\n",
    "        if currentState[0][-7] == 334 or currentState[0][-7] == 1391 or currentState[0][-7] == 6407:\n",
    "            initScore+=0.1\n",
    "        if currentState[0][-8] == 4798 or currentState[0][-8] == 37811 or currentState[0][-8] == 31373:\n",
    "            initScore+=0.1\n",
    "        if currentState[0][-9] == 50280 or currentState[0][-9] == 50286 or currentState[0][-9] == 50272:\n",
    "            initScore+=0.1\n",
    "        if currentState[0][-10] == 201 or currentState[0][-10] == 1441 or currentState[0][-10] == 628:\n",
    "            initScore+=0.1\n",
    "        return initScore\n",
    "\n",
    "    def next_state(self,state,action):\n",
    "        nextState = np.append(state,np.array([[action]]),axis=-1)\n",
    "        return nextState\n",
    "\n",
    "    def is_done_state(self,state,depth):\n",
    "        \n",
    "        if self.isPromptComplete(state,depth):\n",
    "            return True\n",
    "        elif depth>=self.depth:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def get_prompt_from_state(self,state):\n",
    "        with torch.no_grad():\n",
    "            torchState = torch.from_numpy(state).to(device)\n",
    "            prompt_from_state = self.tokenizer.decode(torchState[0])\n",
    "            return prompt_from_state\n",
    "\n",
    "    def getLLMestimates(self,state):\n",
    "        with torch.no_grad():\n",
    "            torchState = torch.from_numpy(state).to(device)\n",
    "            output = self.model(input_ids=torchState)\n",
    "            next_token_logits = output.logits[0, -1, :]\n",
    "            next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            return next_token_probs.detach().cpu().numpy()\n",
    "\n",
    "    def get_best_terminal_state(self,state,depth):\n",
    "        with torch.no_grad():\n",
    "            torchState = torch.from_numpy(state).to(device)\n",
    "            while not self.is_done_state(state,depth):\n",
    "                output = self.model(input_ids=torchState)\n",
    "                next_token_logits = output.logits[0, -1, :]\n",
    "                next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "                sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "                torchState = torch.cat([torchState, sorted_ids[None,0, None]], dim=-1)\n",
    "                state = torchState.detach().cpu().numpy()\n",
    "                depth+=1\n",
    "            return state\n",
    "\n",
    "    def get_montecarlo_return(self,state,depth):\n",
    "        best_terminal_state = self.get_best_terminal_state(state,depth)\n",
    "        complete_prompt = self.get_prompt_from_state(best_terminal_state)\n",
    "        score = self.getPromptScore(best_terminal_state)\n",
    "        #score = self.getPromptScore(complete_prompt)\n",
    "        return score\n",
    "\n",
    "    def get_return(self,state,depth):\n",
    "        ##Sanity Check##\n",
    "        if not self.is_done_state(state,depth):\n",
    "            print(\"Serious error\")\n",
    "            exit(1)\n",
    "        complete_prompt = self.get_prompt_from_state(state)\n",
    "        #score = self.getPromptScore(complete_prompt)\n",
    "        score = self.getPromptScore(state)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe95dc41-a7d9-487b-87cc-5485b3273db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded state 'def hello_world()'\n",
      "decoded state 'def hello_world() {'\n",
      "decoded state 'def hello_world() {\\n'\n",
      "decoded state 'def hello_world() {\\n    '\n",
      "decoded state 'def hello_world() {\\n    return'\n",
      "decoded state 'def hello_world() {\\n    return \"'\n",
      "decoded state 'def hello_world() {\\n    return \"Hello'\n",
      "decoded state 'def hello_world() {\\n    return \"Hello World'\n",
      "decoded state 'def hello_world() {\\n    return \"Hello World!\"'\n",
      "decoded state 'def hello_world() {\\n    return \"Hello World!\";'\n",
      "decoded state 'def hello_world() {\\n    return \"Hello World!\";\\n'\n",
      "decoded state 'def hello_world() {\\n    return \"Hello World!\";\\n}'\n",
      "decoded state 'def hello_world() {\\n    return \"Hello World!\";\\n}\\n'\n",
      "decoded state 'def hello_world() {\\n    return \"Hello World!\";\\n}\\n\\n'\n",
      "Filtered relevant code with stop words ['\\n\\n']-->\n",
      "def hello_world() {\n",
      "    return \"Hello World!\";\n",
      "}\n",
      "\n",
      "2 def hello_world() {\n",
      "3 def hello_world() {\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = LLMQueryEnv(orig_prompt=\"def hello_world()\")\n",
    "init_state = env.get_initial_state()\n",
    "depth=0\n",
    "# print(init_state)\n",
    "### Rollout return ###\n",
    "finalState = env.get_best_terminal_state(init_state,depth)\n",
    "promptGen = env.get_prompt_from_state(finalState)\n",
    "filteredGen=env.trim_with_stopwords(promptGen)\n",
    "\n",
    "print('Filtered relevant code with stop words {}-->\\n{}\\n'.format(env.stopwords, filteredGen))\n",
    "#### Get next best state ###\n",
    "\n",
    "best_prediction = np.argmax(env.getLLMestimates(init_state))\n",
    "next_state = env.next_state(init_state,best_prediction)\n",
    "prompt = env.get_prompt_from_state(next_state)\n",
    "depth+=1\n",
    "print('2',prompt)\n",
    "### Again, get the next best state ###\n",
    "best_prediction = np.argmax(env.getLLMestimates(next_state))\n",
    "next_state = env.next_state(next_state,best_prediction)\n",
    "prompt = env.get_prompt_from_state(next_state)\n",
    "depth+=1\n",
    "print('3',prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb325b8f-6d6c-4cfb-b76c-f84d2225ade3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[[ 4299 23748    62  6894 33529]]\n",
      "decoded state 'def hello_world():'\n",
      "/ext3/miniconda3/envs/codegen/lib/python3.8/site-packages/transformers/models/codegen/modeling_codegen.py:167: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  ../aten/src/ATen/native/TensorCompare.cpp:402.)\n",
      "  attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n",
      "decoded state 'def hello_world():\\n'\n",
      "decoded state 'def hello_world():\\n    '\n",
      "decoded state 'def hello_world():\\n    return'\n",
      "decoded state \"def hello_world():\\n    return '\"\n",
      "decoded state \"def hello_world():\\n    return 'Hello\"\n",
      "decoded state \"def hello_world():\\n    return 'Hello World\"\n",
      "decoded state \"def hello_world():\\n    return 'Hello World!'\"\n",
      "decoded state \"def hello_world():\\n    return 'Hello World!'\\n\"\n",
      "decoded state \"def hello_world():\\n    return 'Hello World!'\\n\\n\"\n",
      "Filtered relevant code with stop words ['\\n\\n']-->\n",
      "def hello_world():\n",
      "    return 'Hello World!'\n",
      "\n",
      "1 def hello_world():\n",
      "\n",
      "2 def hello_world():\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "!python3 LLMQueryEnv.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6943c6b7-252e-464c-a3d3-a7e53267d9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codegen",
   "language": "python",
   "name": "codegen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
